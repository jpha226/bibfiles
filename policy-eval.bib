@article{liu2018representation,
  title={Representation Balancing MDPs for Off-Policy Policy Evaluation},
  author={Liu, Yao and Gottesman, Omer and Raghu, Aniruddh and Komorowski, Matthieu and Faisal, Aldo and Doshi-Velez, Finale and Brunskill, Emma},
  journal={arXiv preprint arXiv:1805.09044},
  year={2018},
  annote={Propose bound for error in MDP model value prediction. Use bound as a surrogate for learning model and show that it leads to lower MSE compared to IS methods.}
}


@article{de2018per,
  title={Per-decision Multi-step Temporal Difference Learning with Control Variates},
  author={De Asis, Kristopher and Sutton, Richard S},
  journal={arXiv preprint arXiv:1807.01830},
  year={2018},
  annote={Add control variates to multi-step TD-learning. Approach is to use the current value function as the control variate.}
}


@inproceedings{liu2018breaking,
  title={Breaking the Curse of Horizon: Infinite-Horizon Off-policy Estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  booktitle={Neural Information Processing Systems (NIPS)},
  year={2018},
  annote={Estimate the state visitation IS ratio in order to break the dependency on the horizon for the variance of the IS estimator.}
}

@inproceedings{sajed2018high,
  title={High-confidence error estimates for learned value functions},
  author={Sajed, Touqir and Chung, Wesley and White, Martha},
  booktitle={Uncertainty in Artificial Intelligence (UAI)},
  year={2018},
  annote={Discusses how to evaluate the quality of a learned value function when you can't compute the exact value-function. Method uses a high-confidence bound on the empirical value-error.}
}


@article{gottesman2018evaluating,
  title={Evaluating Reinforcement Learning Algorithms in Observational Health Settings},
  author={Gottesman, Omer and Johansson, Fredrik and Meier, Joshua and Dent, Jack and Lee, Donghun and Srinivasan, Srivatsan and Zhang, Linying and Ding, Yi and Wihl, David and Peng, Xuefeng and others},
  journal={arXiv preprint arXiv:1805.12298},
  year={2018}
}

@article{bottou2013counterfactual,
  title={Counterfactual reasoning and learning systems: The example of computational advertising},
  author={Bottou, L{\'e}on and Peters, Jonas and Qui{\~n}onero-Candela, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={3207--3260},
  year={2013},
  publisher={JMLR. org}
}

@inproceedings{farajtabar2018more,
  title={More Robust Doubly Robust Off-policy Evaluation},
  author={Farajtabar, Mehrdad and Chow, Yinlam and Ghavamzadeh, Mohammad},
  booktitle={Proceedings of the 35th International Conference on Machine Learning (ICML)},
  year={2018}
}

@inproceedings{thomas2016data-efficient,
  title={Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning},
  author={Thomas, Philip S. and Brunskill, Emma},
  booktitle={Proceedings of the 33rd International Conference on Machine Learning (ICML)},
  year={2016},
  annote={Extend Jiang and Li's doubly robust estimator to infinite horizon problems and extend algorith to weighted DR. Then introduce blended WDR and model based off-policy estimates to further improve performance. Goal is to minimize MSE when evaluating a policy with data from another policy. Model-based and IS estimates are blended in a way similar to complex returns such as the lambda return.}
}

@inproceedings{jiang2016doubly,
  title={Doubly Robust Off-policy Evaluation for Reinforcement Learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={Proceedings of the 33rd International Conference on Machine Learning (ICML)},
  year={2016},
  annote={Alternative to importance sampling with lower variance but still no bias. Extends doubly robust estimator from bandits to MDPs. Idea is to introduce a model to guide IS and reduce variance. Only derived for finite horizon case.}
}

@inproceedings{precup2000eligibility,
	title="Eligibility traces for off-policy policy evaluation",
	author="D. Precup and R. S. Sutton and S. Singh",
	year="2000",
	booktitle="Proceedings of the 17th International Conference on Machine Learning",
	pages="759--766",
}

@inproceedings{Veness2011,
  title={Variance reduction in {M}onte-{C}arlo tree search},
  author={J. Veness and M. Lanctot and M. Bowling},
  booktitle={Proceedings of the 24th Conference on Neural Information Processing Systems},
  pages={1836--1844},
  year={2011}
}

@inproceedings{Zinkevich2006,
  title={Optimal unbiased estimators for evaluating agent performance},
  author={M. Zinkevich and M. Bowling and N. Bard and M. Kan and D. Billings},
  booktitle={Proceedings of the 21st National Conference
on Artificial Intelligence (AAAI)},
  pages="573--578",
  year={2006},
}

@inproceedings{White2009,
  title={Learning a Value Analysis Tool for Agent Evaluation.},
  author={M. White and M. Bowling},
  booktitle="Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI)",
  pages={1976--1981},
  year={2009},
}

	
@inproceedings{clarke2011statistical,
  title={Statistical model checking for cyber-physical systems},
  author={Clarke, Edmund M and Zuliani, Paolo},
  booktitle={International Symposium on Automated Technology for Verification and Analysis},
  pages={1--12},
  year={2011},
  organization={Springer}
}

@book{rubinstein2013cross,
  title={The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning},
  author={Rubinstein, Reuven Y and Kroese, Dirk P},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{ahamed2006adaptive,
  title={Adaptive importance sampling technique for Markov chains using stochastic approximation},
  author={Ahamed, TP Imthias and Borkar, Vivek S and Juneja, S},
  journal={Operations Research},
  volume={54},
  number={3},
  pages={489--504},
  year={2006},
  publisher={INFORMS}
}

@inproceedings{frank2008reinforcement,
  title={Reinforcement learning in the presence of rare events},
  author={Frank, Jordan and Mannor, Shie and Precup, Doina},
  booktitle={Proceedings of the 25th International Conference on Machine learning},
  pages={336--343},
  year={2008},
  organization={ACM},
  annote={Learn value function in the presence of rare-events. Assume known rare-events and a simulator which directly controls the probability of rare-events. Can change the probability of rare-events and use importance sampling to correct distribution shift. Introduce algorithms that learn in this way and compare favorably to TD-learning. Analyze tabular bias and variance compared to TD-learning.}
}

@inproceedings{desai2001simulation,
  title={Simulation in optimization and optimization in simulation: {A} {M}arkov chain perspective on adaptive {M}onte {C}arlo algorithms},
  author={Desai, Paritosh Y and Glynn, Peter W},
  booktitle={Proceedings of the 33rd conference on Winter simulation},
  pages={379--384},
  year={2001},
  organization={IEEE Computer Society}
}

@article{rubinstein1999cross,
  title={The cross-entropy method for combinatorial and continuous optimization},
  author={Rubinstein, Reuven},
  journal={Methodology and computing in applied probability},
  volume={1},
  number={2},
  pages={127--190},
  year={1999},
  publisher={Springer}
}

@inproceedings{duan2016benchmarking,
  title={Benchmarking Deep Reinforcement Learning for Continuous Control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={In Proceedings of the 33rd International Conference on Machine Learning},
  year={2016},
  annote={Compares a bunch of state-of-the-art RL algorithms on a bunch of tasks. Tasks range from standard control to high dimensional control, tasks with partial observability, hierarchical tasks, and system identification tasks. Overall good codebase released with the paper and a good reference for several RL algorithms.}
}

@inproceedings{thomas2015off-policy,
        title={High Confidence Off-Policy Evaluation},
        author={Thomas, Philip S. and Theocharous, Georgios and Ghavamzadeh, Mohammad},
        year={2015},
        booktitle={Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI)},
        annote={This paper proposes a method for off-policy evaluation with high confidence in the lower bound of the evaluated policy's performance. Given trajectories from other policies, importance sampling can be used to estimate the expected value of a policy. Concentration inequalities can be used to get a lower bound on the expected return. This paper proposes a new concentration inequality specifically for the policy evaluation setting. The paper reviews a few existing concentration inequalities and then combines two of them to obtain a new tight bound which is applicable to non identically distributed random variables (e.g. returns from different policies) and doesn't depend on the range of the variables. The bound is proven and experimental results show its performance.}
}

@phdthesis{bastani2014model,
  title={Model-free intelligent diabetes management using machine learning},
  author={Bastani, Meysam},
  year={2014},
  school={Masterâ€™s thesis, Department of Computing Science, University of Alberta}
}

@inproceedings{theocharous2015personalized,
  title={Personalized Ad Recommendation Systems for Life-Time Value Optimization with Guarantees.},
  author={Theocharous, Georgios and Thomas, Philip S. and Ghavamzadeh, Mohammad},
  booktitle={Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI)},
  pages={1806--1812},
  year={2015}
}

% 50
@article{lillicrap2015continuous,
        author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
        title={Continuous control with deep reinforcement learning},
        journal={CoRR},
        volume={abs/1509.02971},
        year={2015}
}

@inproceedings{schulman2015trust,
        title={Trust Region Policy Optimization},
        author={Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
        year={2015},
        booktitle={Proceedings of the 32nd International Conference on Machine Learning ( ICML)}
}

@inproceedings{greensmith2001variance,
  title={Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning.},
  author={Greensmith, Evan and Bartlett, Peter L and Baxter, Jonathan and others},
  booktitle={Proceedings of the 14th Conference on Neural Information Processing Systems (NIPS)},
  pages={1507--1514},
  year={2001},
  annote={Analyze the baseline technique used to reduce variance of policy gradient methods. Mainly theoretical results on variance with some preliminary experimental work.}
}

@inproceedings{kakade2001natural,
  title={A Natural Policy Gradient.},
  author={Kakade, Sham},
  booktitle={Proceedings of the 14th Conference on Neural Information Processing Systems (NIPS)},
  volume={14},
  pages={1531--1538},
  year={2001},
  annote={Natural gradient policy improvement method. Show that each update moves policy towards optimal action not just a better action.}
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine Learning},
  volume={49},
  number={2-3},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

@inproceedings{sutton2000policy,
        title={Policy Gradient Methods for Reinforcement Learning with Function Approximation},
        author={Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
        year={2000},
        booktitle={Proceedings of the 13th Conference on Neural Information Processing Systems (NIPS)}
}

@article{Hammersley1964,
  title={Monte {C}arlo Methods, Methuen \& Co},
  author={Hammersley, JM and Handscomb, DC},
  journal={Ltd., London},
  pages={40},
  year={1964}}
  
@article{strehl2009reinforcement,
  title={Reinforcement learning in finite MDPs: P{A}{C} analysis},
  author={Strehl, Alexander L. and Li, Lihong and Littman, Michael L.},
  journal={Journal of Machine Learning Research},
  volume={10},
  pages={2413--2444},
  year={2009},
}

@inproceedings{van2014true,
  title={True Online {T}{D} ($\lambda$).},
  author={van Seijen, Harm and Sutton, Richard S},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML)},
  volume={14},
  pages={692--700},
  year={2014}
}

@book{sen1993large,
title={Large Sample Methods in Statistics: An Introduction with Applications},
author={Sen, P.K. and Singer, J.M.},
publisher={Chapman \& Hall},
year={1993},
annote={Law of large numbers}
}

@article{bertsekas2000gradient,
title={Gradient Convergence in Gradient Methods With Erros},
publisher={Society for Industrial and Applied Mathematics},
author={Bertsekas, Dimitri P. and Tsitsiklis, John N.},
year={2000},
volume={10},
pages={627-642},
annote={Grad descent convergence conditions}
}

@inproceedings{li2015toward,
  title={Toward minimax off-policy value estimation},
  author={Li, Lihong and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  year={2015},
  booktitle={Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS)}
}

@inproceedings{liu2017black,
  title={Black-box importance sampling},
  author={Liu, Qiang and Lee, Jason D},
  booktitle={Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2017}
}

@inproceedings{hanna2017data-efficient,
  title={Data-efficient policy evaluation through behavior policy search},
  author={Hanna, Josiah and Thomas, Philip S. and Stone, Peter and Niekum, Scott},
  year={2017},
  booktitle={Proceedings of the 34th International Conference on Machine Learning (ICML)},
  annote={Optimal behavior policy for IS is not the evaluation policy. Propose an algorithm to learn a lower variance policy for IS.}
}

@inproceedings{hanna2017bootstrapping,
  title={Bootstrapping with models: Confidence intervals for off-policy evaluation},
  author={Hanna, Josiah P and Stone, Peter and Niekum, Scott},
  booktitle={Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
  pages={538--546},
  year={2017},
  organization={International Foundation for Autonomous Agents and Multiagent Systems},
  annote={Propose two bootstrap confidence interval methods. One uses model-based policy evaluation and the other uses weighted doubly robust. Outperform similar methods with IS as base policy evaluation method.}
}


@article{brockman2016openai,
  title={OpenAI gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L.},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{gruslys2017reactor,
  title={The Reactor: A Sample-Efficient Actor-Critic Architecture},
  author={Gruslys, Audrunas and Azar, Mohammad Gheshlaghi and Bellemare, Marc G. and Munos, Remi},
  journal={arXiv preprint arXiv:1704.04651},
  year={2017}
}

@inproceedings{levine2013guided,
  title={Guided policy search},
  author={Levine, Sergey and Koltun, Vladlen},
  booktitle={Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
  pages={1--9},
  year={2013}
}


@inproceedings{mandel2016offline,
  title={Offline Evaluation of Online Reinforcement Learning Algorithms.},
  author={Mandel, Travis and Liu, Yun-En and Brunskill, Emma and Popovic, Zoran},
  booktitle={Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI)},
  pages={1926--1933},
  year={2016}
}

@inproceedings{fonteneau2010model,
  title={Model-free Monte Carlo-like policy evaluation},
  author={Fonteneau, Raphael and Murphy, Susan and Wehenkel, Louis and Ernst, Damien},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={217--224},
  year={2010}
}

@inproceedings{hallak2015off,
  title={Off-policy model-based learning under unknown factored dynamics},
  author={Hallak, Assaf and Schnitzler, Francois and Mann, Timothy and Mannor, Shie},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  pages={711--719},
  year={2015}
}

@inproceedings{mishra2017prediction,
  title={Prediction and Control with Temporal Segment Models},
  author={Mishra, Nikhil and Abbeel, Pieter and Mordatch, Igor},
  year={2017},
  booktitle={Proceedings of the 34th International Conference on Machine Learning (ICML)}
}


@inproceedings{ross2012agnostic,
	title={Agnostic System Identification for Model-Based Reinforcement Learning},
	author={Ross, Stephane and Bagnell, J. Andrew},
	year={2012},
	booktitle={29th International Conference on Machine Learning, ICML},
	annote={This paper extends the DAgger method to model-based reinforcement learning. The main contributions are strong guarantees even if the true systemis not in the class of models. The results also formalize the iterative system identification / controller synthesis method of learning a model, designing a controller, and then modifying the model after collecting data under the new controller. The method works by iteratively building a model, learning a controller, building a new model with the new data and the old data and then producing a new controller. This can be repeated for a fixed number of iterations. The problem with not covering enough of the state space is that any model will under estimate the cost of the part of the state space where it doesn't have data. This makes controllers that visit these parts of the state space more likely. The method needs an exploration distribution to sample transitions from and it isn't clear if this involves random access to the system or refers to rollouts.}
}

@inproceedings{munos2016safe,
  title={Safe and efficient off-policy reinforcement learning},
  author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1054--1062},
  year={2016}
}

@phdthesis{thomas2015safe,
	title={Safe Reinforcement Learning},
	author={Thomas, Philip S.},
	year={2015},
	school={University of Massachusetts Amherst}
}


@article{thomas2016magical,
  title={Magical Policy Search: Data Efficient Reinforcement Learning with Guarantees of Global Optimality},
  author={Thomas, Philip S and Brunskill, Emma},
  journal={European Workshop On Reinforcement Learning},
  year={2016}
}

@article{shimodaira2000improving,
  title={Improving predictive inference under covariate shift by weighting the log-likelihood function},
  author={Shimodaira, Hidetoshi},
  journal={Journal of statistical planning and inference},
  volume={90},
  number={2},
  pages={227--244},
  year={2000},
  publisher={Elsevier}
}

@inproceedings{dudik2011doubly,
  title={Doubly robust policy evaluation and learning},
  author={Dud{\'\i}k, Miroslav and Langford, John and Li, Lihong},
  booktitle={Proceedings of the 28th International Conference on International Conference on Machine Learning},
  pages={1097--1104},
  year={2011},
  organization={Omnipress}
}

@inproceedings{narita2019efficient,
  title={Efficient Counterfactual Learning from Bandit Feedback},
  author={Narita, Yusuke and Yasui, Shota and Yata, Kohei},
  year={2019},
  booktitle={The 35th AAAI Conference on Artificial Intelligence (AAAI)},
  annote={Estimate the behavior policy even if you know the true one for contextual bandits. Two theoretical results showing lower asymptotic variance and empirical results with web marketing.}
}

@inproceedings{strehl2010learning,
  title={Learning from logged implicit exploration data},
  author={Strehl, Alex and Langford, John and Li, Lihong and Kakade, Sham M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2217--2225},
  year={2010},
  annote={Estimate behavior policy to smooth importance weights when logging with deterministic policies.}
}

@inproceedings{swaminathan2015self,
  title={The self-normalized estimator for counterfactual learning},
  author={Swaminathan, Adith and Joachims, Thorsten},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3231--3239},
  year={2015},
  annote={weighted importance sampling for bandit learning}
}

@inproceedings{doroudi2017importance,
  title={Importance sampling for fair policy selection},
  author={Doroudi, Shayan and Thomas, Philip S and Brunskill, Emma},
  booktitle={Uncertainty in Artificial Intelligence (UAI)},
  year={2017},
  annote={Importance sampling is unbiased but tends to under-estimate the value of policies in MDPs.}
}
